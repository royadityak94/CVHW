{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from extractDigitFeatures import extractDigitFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.loadmat('../data/digits-normal.mat') \n",
    "features = extractDigitFeatures(data['x'], 'hog')\n",
    "#model = trainModel(features[:, data['set']==trainSet], data['y'][data['set']==trainSet])\n",
    "trainSet, testSet = 1, 2\n",
    "param = {}\n",
    "param['lambda'], param['maxiter'], param['eta'] = 0.01, 200,  .01\n",
    "# trainModel : x = features[data['set']==trainSet, :], y = data['y'][data['set']==trainSet]\n",
    "x_train = features[:, data['set']==trainSet]\n",
    "y_train = data['y'][data['set']==trainSet]\n",
    "\n",
    "x_val = features[:, data['set']==3]\n",
    "y_val = data['y'][data['set']==3]\n",
    "\n",
    "x_test = features[:, data['set']==testSet]\n",
    "y_test = data['y'][data['set']==testSet]\n",
    "\n",
    "# multiclassLRTrain : x, y, param\n",
    "classLabels = np.unique(y)\n",
    "numClass = classLabels.shape[0]\n",
    "numFeats = x.shape[0]\n",
    "numData = x.shape[1]\n",
    "\n",
    "# Initialize weights randomly (Implement gradient descent)\n",
    "#model = {}\n",
    "#model['w'] = np.random.randn(numClass, numFeats)*0.01\n",
    "#model['classLabels'] = classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 2000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e = numpy.exp(x - numpy.max(x))  # prevent overflow\n",
    "    if e.ndim == 1:\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    else:  \n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "class LogisticRegression11(object):\n",
    "    def __init__(self, input, label, n_in, n_out):\n",
    "        self.x = input\n",
    "        self.y = label\n",
    "        self.W = numpy.zeros((n_in, n_out))  # initialize W 0\n",
    "        self.b = numpy.zeros(n_out)          # initialize bias 0\n",
    "\n",
    "        # self.params = [self.W, self.b]\n",
    "\n",
    "    def train(self, lr=0.1, input=None, L2_reg=0.00):\n",
    "        if input is not None:\n",
    "            self.x = input\n",
    "\n",
    "        # p_y_given_x = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "        p_y_given_x = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "        d_y = self.y - p_y_given_x\n",
    "        \n",
    "        self.W += lr * numpy.dot(self.x.T, d_y) - lr * L2_reg * self.W\n",
    "        self.b += lr * numpy.mean(d_y, axis=0)\n",
    "        \n",
    "        # cost = self.negative_log_likelihood()\n",
    "        # return cost\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        # sigmoid_activation = sigmoid(numpy.dot(self.x, self.W) + self.b)\n",
    "        sigmoid_activation = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "\n",
    "        cross_entropy = - numpy.mean(\n",
    "            numpy.sum(self.y * numpy.log(sigmoid_activation) +\n",
    "            (1 - self.y) * numpy.log(1 - sigmoid_activation),\n",
    "                      axis=1))\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # return sigmoid(numpy.dot(x, self.W) + self.b)\n",
    "        return softmax(numpy.dot(x, self.W) + self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1000,392) and (1000,10) not aligned: 392 (dim 1) != 1000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-ddf083e2caa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Training epoch %d, cost is '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-aa54404798c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, input, L2_reg)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# p_y_given_x = sigmoid(numpy.dot(self.x, self.W) + self.b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mp_y_given_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0md_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_y_given_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1000,392) and (1000,10) not aligned: 392 (dim 1) != 1000 (dim 0)"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression11(input=x_train.T, label=y_train, n_in=x.T.shape[0], n_out=10)\n",
    "\n",
    "# train\n",
    "for epoch in range(param['maxiter']):\n",
    "    classifier.train(lr=param['eta'])\n",
    "    cost = classifier.negative_log_likelihood()\n",
    "    print ('Training epoch %d, cost is ' % epoch, cost)\n",
    "    #learning_rate *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 500), (1000,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/cs682/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/cs682/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(max_iter=1500)\n",
    "logisticRegr.fit(x_train.T, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logisticRegr.predict(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y, y_pred):\n",
    "    correct_pred = 0\n",
    "    for itr in range(y.size):\n",
    "        if y_pred[itr] == y[itr]:\n",
    "            correct_pred +=1\n",
    "    return correct_pred / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 2, 2, 2, 8, 0, 2, 2, 2,\n",
       "       2, 2, 9, 8, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 0, 3, 3, 9, 2, 0, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 8, 3, 3, 3, 3, 3, 3, 3, 9, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 4, 4, 4, 4, 5, 9, 4, 4, 2, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0,\n",
       "       8, 4], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:222]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs682",
   "language": "python",
   "name": "cs682"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
